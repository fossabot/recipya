"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[3219],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>m});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var p=r.createContext({}),l=function(e){var t=r.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=l(e.components);return r.createElement(p.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,p=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=l(n),h=a,m=u["".concat(p,".").concat(h)]||u[h]||d[h]||o;return n?r.createElement(m,i(i({ref:t},c),{},{components:n})):r.createElement(m,i({ref:t},c))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=h;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[u]="string"==typeof e?e:a,i[1]=s;for(var l=2;l<o;l++)i[l]=n[l];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}h.displayName="MDXCreateElement"},3956:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var r=n(7462),a=(n(7294),n(3905));const o={},i="Support a Website",s={unversionedId:"development/workflow/import-website",id:"development/workflow/import-website",title:"Support a Website",description:"You to understand how the scraper works to support a website.",source:"@site/docs/development/workflow/import-website.md",sourceDirName:"development/workflow",slug:"/development/workflow/import-website",permalink:"/recipya/docs/development/workflow/import-website",draft:!1,editUrl:"https://github.com/reaper47/recipya/tree/main/website/docs/docs/development/workflow/import-website.md",tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Add an Endpoint",permalink:"/recipya/docs/development/workflow/add-endpoint"}},p={},l=[{value:"The Scraper",id:"the-scraper",level:2},{value:"Workflow",id:"workflow",level:2},{value:"Database",id:"database",level:3},{value:"Test",id:"test",level:3},{value:"The Go Code",id:"the-go-code",level:3}],c={toc:l},u="wrapper";function d(e){let{components:t,...n}=e;return(0,a.kt)(u,(0,r.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"support-a-website"},"Support a Website"),(0,a.kt)("p",null,"You to understand how the scraper works to support a website.\nThen, we will guide you through adding a website to the supported list with an example."),(0,a.kt)("h2",{id:"the-scraper"},"The Scraper"),(0,a.kt)("p",null,"Recipya developed its own recipe scraper, which resides in the ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/reaper47/recipya/tree/main/internal/scraper"},"internal/scraper"),"\npackage. This scraper uses ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/PuerkitoBio/goquery"},"goquery")," to extract information from web pages.\nIts main file is ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/reaper47/recipya/blob/main/internal/scraper/scraper.go"},"scraper.go"),". You will find a single exposed function named ",(0,a.kt)("inlineCode",{parentName:"p"},"Scrape"),", which\ntakes a URL and a ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/reaper47/recipya/blob/main/internal/services/service.go#L102"},"files service")," as parameters. The files services is an interface with functions to\nmanipulate files in the OS. The use of an interface simplifies the process of mocking file operations during testing."),(0,a.kt)("p",null,"You can read how the function works, but essentially it involves fetching the web page using Go's HTTP client,\ncreating a ",(0,a.kt)("inlineCode",{parentName:"p"},"goquery")," document from the response, extracting into a\n",(0,a.kt)("a",{parentName:"p",href:"https://github.com/reaper47/recipya/blob/main/internal/models/schema-recipe.go"},"models.RecipeSchema")," struct, uploading\nthe image to the server, and finally returning the recipe schema model. The image is compressed and resized.\nWhether compression is too high remains is subject to evaluation."),(0,a.kt)("h2",{id:"workflow"},"Workflow"),(0,a.kt)("p",null,"Let's assume a user requests ",(0,a.kt)("a",{parentName:"p",href:"https://www.example.com/recipes/declicious-bbq-steak"},"https://www.example.com/recipes/declicious-bbq-steak")," to be supported.\nThis section will help you understand how to add this website to the list of supported sites."),(0,a.kt)("h3",{id:"database"},"Database"),(0,a.kt)("p",null,"Initially, a SQLite migration file needs to be created using Goose to insert the desired website into the\nwebsites table. To do so, open a terminal and navigate to ",(0,a.kt)("inlineCode",{parentName:"p"},"internal/services/migrations"),". Then, generate\nthe migration file."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"goose create support-website sql\n")),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"support-website")," is the name of the migration. It can be anything else. The command will create a new file of the\nform ",(0,a.kt)("inlineCode",{parentName:"p"},"timestamp_name-of-migration.sql"),". It will be embedded into the executable on build and will be executed when the\nuser starts the server. "),(0,a.kt)("p",null,"The final step involves inserting the website into the database:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sql"},"-- +goose Up\nINSERT INTO websites (host, url) \nVALUES ('example.com', 'https://www.example.com/recipes/declicious-bbq-steak');\n\n-- +goose Down\nDELETE FROM websites\nWHERE host = 'example.com' \n  AND url = 'https://www.example.com/recipes/declicious-bbq-steak';\n")),(0,a.kt)("p",null,"The host field could eventually be removed because we can determine it from Go using\nthe ",(0,a.kt)("a",{parentName:"p",href:"https://pkg.go.dev/net/url#URL.Hostname"},"net/url")," package."),(0,a.kt)("h3",{id:"test"},"Test"),(0,a.kt)("p",null,"Setting up a test involves accessing the website and creating a test case within ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/reaper47/recipya/blob/main/internal/scraper/scraper_test.go"},"internal/scraper/scraper_test.go"),".\nOpen this file. In the ",(0,a.kt)("inlineCode",{parentName:"p"},"testcases")," slice, locate the test where the ",(0,a.kt)("inlineCode",{parentName:"p"},"name")," field struct starts with ",(0,a.kt)("inlineCode",{parentName:"p"},"E"),".\nThe tests are listed alphabetically so insert your ",(0,a.kt)("inlineCode",{parentName:"p"},'name: "example.com"')," test where appropriate. You can use an\nexisting struct as a template."),(0,a.kt)("p",null,"Next, alternate between the recipe web page and the test to modify the ",(0,a.kt)("inlineCode",{parentName:"p"},"models.RecipeSchema")," of\nthe ",(0,a.kt)("inlineCode",{parentName:"p"},"want")," field. You can proceed to writing code once the setup is done. "),(0,a.kt)("p",null,"Executing the test by clicking the green play button to the left it should confirm its failure. "),(0,a.kt)("h3",{id:"the-go-code"},"The Go Code"),(0,a.kt)("p",null,"The initial step is to include the ",(0,a.kt)("inlineCode",{parentName:"p"},"example.com")," case within the list of supported websites. To achieve this, open\n",(0,a.kt)("a",{parentName:"p",href:"https://github.com/reaper47/recipya/blob/main/internal/scraper/websites.go"},"internal/scraper/websites.go"),". This file\ncontains the ",(0,a.kt)("inlineCode",{parentName:"p"},"scrapeWebsite")," function, which executes the relevant scrape function for the parsed\nHTML web page. Your task involves adding the host within the switch-case block. Therefore, add ",(0,a.kt)("inlineCode",{parentName:"p"},'case "example"')," to the switch-case block\nof ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/reaper47/recipya/blob/main/internal/scraper/websites.go#L90"},"case 'e'"),". "),(0,a.kt)("p",null,"Following this, the body of the case must be added. Each case calls a function that scrapes the parsed HTML document\none of the following three methods."),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("inlineCode",{parentName:"li"},"parseLdJSON"),": Searches and parses JSON-LD metadata, frequently containing recipe information because many sites use WordPress."),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("inlineCode",{parentName:"li"},"parseGraph"),": Searches and parses JSON-LD metadata containing a ",(0,a.kt)("inlineCode",{parentName:"li"},"@graph")," field."),(0,a.kt)("li",{parentName:"ol"},"Custom parser when the above functions fail.")),(0,a.kt)("p",null,"A future improvement would merge ",(0,a.kt)("inlineCode",{parentName:"p"},"parseLdJSON")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"parseGraph")," to reduce the number of ways to scrape a website."),(0,a.kt)("p",null,"Inspect the source of the recipe web page for occurrences of ",(0,a.kt)("inlineCode",{parentName:"p"},"ld+json"),". If found,\nthen write ",(0,a.kt)("inlineCode",{parentName:"p"},"return parseLdJSON(doc)")," and run the test. If it fails or the function seems not to work, then\ntry ",(0,a.kt)("inlineCode",{parentName:"p"},"return parseGraph(doc)"),". If these attempts fall short, then you must write a custom scraper."),(0,a.kt)("p",null,"The nomenclature of this function is ",(0,a.kt)("inlineCode",{parentName:"p"},"scrape{Host}"),". In your case, it would be ",(0,a.kt)("inlineCode",{parentName:"p"},"return scrapeExample(doc)"),".\nThen, create a new file named ",(0,a.kt)("inlineCode",{parentName:"p"},"example.go")," under ",(0,a.kt)("inlineCode",{parentName:"p"},"internal/scraper")," and add the\n",(0,a.kt)("inlineCode",{parentName:"p"},"func scrapeKuchniadomova(root *goquery.Document) (models.RecipeSchema, error)")," function. Please check any other\ncustom scraper files to understand how to write your own."),(0,a.kt)("p",null,"Congratulations! That is all you need to know to support a website. Feel free to open a PR once your scrape function is\nwritten and the tests pass."))}d.isMDXComponent=!0}}]);